---
title: "Publication notebook"
output: html_notebook
---

This includes the analysis and figures that will go into the draft publication of the central WAIS trends article.
This will also be a space for drafting and iterating on the wording for methods/analysis/interpretation description.

## METHODS

### Data imports and preprocessing

Load libraries:
```{r}
library(here)
library(INLA)
library(dplyr)
library(tidyr)
library(broom)
library(ggplot2)
library(colorspace)
library(spdep)
```

```{r}
data.all = readRDS(here('data/Rdata-clean.rds'))
gdf_traces = readRDS(here('data/Rdata-gdf_trace.rds'))

# # Clip bounds
# bbox = st_bbox(gdf_traces)
# clipper = c(
#   bbox[1]+0.1*(bbox[3]-bbox[1]),
#   bbox[2]+0.5*(bbox[4]-bbox[2]),
#   0.6*(bbox[3]-bbox[1])+bbox[1], 
#   bbox[4]
# )
# gdf_traces = st_crop(gdf_traces, clipper)

# Remove locations with less than 10 years of data
trace.keep = data.all %>% count(trace_ID) %>% filter(n>=10)
data.all = data.all %>% filter(trace_ID %in% trace.keep$trace_ID)
gdf_traces = gdf_traces %>% filter(trace_ID %in% trace.keep$trace_ID)

# Subset data to every 5th location (coarsens data to ~1 km)
skip.int = 5
gdf.idx = seq(1, nrow(gdf_traces), by=skip.int)
gdf_traces = gdf_traces[gdf.idx,]

# Filter data to subsetted period
data = data.all %>% filter(trace_ID %in% gdf_traces$trace_ID) %>%
  filter(Year >= 1975) %>% filter(Year < 2015) %>% arrange(trace_ID, Year)

# Remove gdf rows where all data have been filtered out
gdf_traces = gdf_traces %>% filter(trace_ID %in% data$trace_ID)

# Select variables of interest
dat = data %>% select(trace_ID, East, North, Year, accum, 
                      std, SMB, elev, dem, slope, aspect, 
                      # u10m, v10m, mu.u10, mu.v10
                      )

# Center covariates
dat = dat %>% 
  mutate(elev = elev-mean(elev, na.rm=TRUE), 
         dem=dem-mean(dem, na.rm=TRUE), slope=slope-mean(slope, na.rm=TRUE), 
         # u10m=u10m-mean(u10m, na.rm=TRUE), v10m=v10m-mean(v10m, na.rm=TRUE), 
         # mu.u10=mu.u10-mean(mu.u10, na.rm=TRUE), mu.v10=mu.v10-mean(mu.v10, na.rm=TRUE)
         ) %>% 
  mutate(Year.mod = Year-mean(Year), Year.idx = (Year-min(Year)+1))
         # u10m=mean(u10m, na.rm=TRUE), v10m=mean(v10m, na.rm=TRUE))

# Scale covariates by sd
dat = dat %>% 
  mutate(elev = elev/sd(elev), dem=dem/sd(dem), 
         slope=slope/sd(slope), 
         # u10m=u10m/sd(u10m), v10m=v10m/sd(v10m), mu.u10=sd(mu.u10), mu.v10=sd(mu.v10)
         )

# Split into training and testing sets
dat = dat %>% mutate(row.ID = 1:nrow(dat)) %>% relocate(row.ID)
dat.train = dat %>% slice_sample(prop = 0.80) %>% arrange(row.ID)
dat.test = dat %>% filter(!row.ID %in% dat.train$row.ID)
```

Directly calculate linear coefficients of time from raw data.
```{r}
yr.trends = dat.train %>% 
  group_by(trace_ID) %>% 
  do(tidy(lm(accum ~ Year.mod, data = .))) %>% 
  filter(term=='Year.mod') %>% select(-term)
dat.mu = dat.train %>% group_by(trace_ID) %>% 
  summarize(East=mean(East), North=mean(North), accum=mean(accum)) %>% 
  left_join(yr.trends %>% select(trace_ID, estimate)) %>% 
  mutate(log.est = log(1+(estimate/accum)))
```

### Model setup

The accumulation data represent a continuous distribution of a random variable over two continuous spatial dimensions and indexed over an additional discrete temporal dimension (e.g. annually resolved time series).
We therefore choose to model accumulation as a Gamma distribution to ensure a zero-bounded function with increased variance at higher expected values as also observed in the raw data.
This furthermore matches established literature where precipitation values are frequently assumed to follow a Gamma distribution **(citations)**.
We express this distribution in terms of the expected accumulation for an arbitrary space location and time point $\mu_{s,t}$ and a precision parameter at the corresponding time $\phi_t$ 
$$\mathbf{Y} \sim Gamma(\mu_{s,t},\phi_t)$$
where the variance at an arbitrary spacetime point is
$$Var(s_t) = \frac{\mu_{s,t}^2}{\phi_t}$$

For each observation $y_{s,t} \in \mathbf{Y}$, we construct a linear predictor $\eta_{s,t}$ associated with the expectation value $\mu_{s,t}$ via a log link function.
$$\log(\mu_{s,t}) = \eta_{s,t}$$

We further model $\eta_{s,t}$ as a generalized linear equation consisting of fixed effect variables $\mathbf{X}$ and smoothed random latent effects (in this case modeling a spatial random effect $u(\cdot)$ and a temporal random effect $\omega(\cdot)$).
To explicitly model $u(\cdot)$ as a trend in accumulation over time, we further index this random variable by year.
The equation for our generalized linear model is therefore:
$$
\eta_{s,t} = \alpha + \sum^M_{m=1} \beta_m x_{m,s} + u(s)\times t + \omega(t)
$$

This model therefore accounts for fixed global effects, a spatially-varying latent effect indexed on time (i.e. providing explicit estimates of trends in annual acculation), and a temporally-varying latent effect.

The latent random effects $u$ and $\omega$ are indexed by a set of parameters $\theta_s$ and $\theta_t$ that account for the spatial and temporal correlation in the data.
We model these correlation structures (as defined by the parameters $\pmb{\theta}$) as latent stationary Gaussian fields, using a function of the hyperparameters $\pmb{\psi}$ and an associated prior distribution $p(\pmb{\psi})$.
This is equivalent to assuming that $\pmb{\theta}$ is associated with some multivariate Normal distribution with mean $\mu = (\mu_1,...,\mu_n)'$ and covariance structure $\Sigma$, where $\Sigma_{jk} = Cov(\theta_j, \theta_k)$.

$$
\pmb{\theta} \sim \mathcal{N}(\mu_\theta, \Sigma_\theta)
$$

By assuming adherence to Markovian properties, the precision matrix $Q = \Sigma^{-1}$ is sparse, with the non-zero components of $Q$ completely given by the neighborhood structure ($N$) of the process, i.e. $Q_{jk} \neq 0 \iff k \in \{j,N(j)\}$.
The specification of a Gaussian-Markov Random Field for both the spatial and temporal structures permits vast improvements in computational efficiency.
The choice of a separable space/time model (where the spatial and temporal random effects are modeled independently) also greatly improves computation time but will fail to capture any co-dependencies between these effects.

We model the spatial random effect as a stochastic partial differential equation...

We model the temporal random effect $\omega$ (representing how the mean spatial field evolves in time) as a second-order autoregressive model where
$$\omega_t = \rho \omega_{t-1}$$
An AR(2) process was selected due to the observed temporal autocorrelation in the data, which indicates that accumulation retains autocorrelation up to a 2-year lag time.



The modeling objective, therefore, is to find the joint posterior distribution of the various parameters of interest, permitting inference over the processes governing accumulation in the region and making predictions at unobserved locations possible.
$$P(\phi, \alpha, \pmb{\beta}, \rho, R_s, \sigma_s \mid \mathbf{Y})$$



```{r}
# More coarse mesh for development purposes
mesh = inla.mesh.2d(loc = dat.train %>% select(East, North),
                    max.edge = c(25000, 100000), cutoff = 1000)
# mesh = inla.mesh.2d(loc = dat.train %>% select(East, North),
#                     max.edge = c(30000, 90000), cutoff = 2000)
plot(mesh)
points(dat.train %>% select(East, North), col = "red")

# 2D spatial Matern GMRF via SPDE (with assigned priors)
spde = inla.spde2.pcmatern(mesh = mesh, 
  prior.range = c(25000, 0.01), # P(range < 20 km) = 0.01
  prior.sigma = c(1, 0.01)) #P(sd > 1) = 0.05

# Locator index for spatial random effect
spat.idx <- inla.spde.make.index('spat.idx', spde$n.spde)

# Projector matrix for spatial locations (and indexed on Year for trends)
A.spat <- inla.spde.make.A(mesh, 
                         loc=cbind(dat.train$East, dat.train$North), 
                         weights = dat.train$Year.mod)

# Make data stack
dat.stack <- inla.stack(
  data = list(y = dat.train$accum), 
  A = list(1, 1, 1, A.spat),
  effects = list(list(Intercept = rep(1, nrow(dat.train))), 
                 tibble(elev = dat.train$elev, dem=dat.train$dem, 
                        slope=dat.train$slope, aspect=dat.train$aspect),
                 list(time = dat.train$Year.mod),
                 spat.idx),
  tag = 'dat')

# Prior for temporal autocorrelation
time.spec = list(rho = list(prior = 'pc.cor1', param = c(0.3, 0.95)))

# Model formula
f.mod = y ~ -1 + Intercept + #Global intercept
  dem + #Fixed effects
  # f(dem, model='clinear', range=c(-Inf,0)) + #Fixed effects
  f(time, model = 'ar1', hyper = time.spec) +  #Temporal random effect modeled as AR1
  f(spat.idx, model = spde) #Spatial randomed effect (indexed by Year)
```

Run and summarize model
```{r}
# Model formula
mod = inla(f.mod,
           data = inla.stack.data(dat.stack),
           family = 'Gamma',
           control.predictor = list(compute = TRUE, A = inla.stack.A(dat.stack), link=1),
           # control.inla = list(int.strategy='eb'), #Improves comp time at cost of accuracy/precision
           control.compute = list(waic=TRUE, config=TRUE))

n.iter = 1
while (mod$mode$mode.status>0 && n.iter<=3) {
  print("Issues with Hessian. Re-running to solve negative eigenalues in the Hessian")
  n.iter = n.iter + 1
  print(paste("Starting Iteration", n.iter, "of INLA..."))
  mod = inla.rerun(mod)
}

if (mod$mode$mode.status > 0) {
  print("Issues with Hessian eigenvalues persist. Treat model results with suspicion!")
}
```

```{r}
summary(mod)
```

## Model results

```{r}
# Helper function to extract and transform posteriors
t.marg = function(mod.marg, t.func, param.names=NULL, n=100) {
  
  if (any(class(mod.marg) == "matrix")) {
    mod.marg = list(mod.marg)
    if (!is.null(param.names)) {
      names(mod.marg = param.names)
    } else {
      names(mod.marg) = "UNNAMED"
    }
  }
  
  if (is.null(param.names)) {
    param.names = names(mod.marg)
  }
  
    if (is.list(t.func) != TRUE) {
    t.func = list(t.func)
  }
  if (length(t.func) == 1) {
    t.func = rep(t.func, length(param.names))
  }
  
  # Preallocate output tbl
  out = tibble(x=numeric(), y=numeric(), Param=vector(mode="character"))
  
  marg.names = names(mod.marg)
  
  for (i in 1:length(marg.names)) {
    marg.i = inla.tmarginal(t.func[[i]], mod.marg[[marg.names[i]]], n=n)
    out = out %>% bind_rows(tibble(x=marg.i[,1], y=marg.i[,2], Param=param.names[i]))
  }
  return(out)
}
```

```{r}
marg.fixed = t.marg(mod$marginals.fixed, function(x) exp(x))
HP.names = c("Gamma precision", 
             "Time precision", "Rho for time", 
             "Range for spat.idx", "Stdev for spat.idx")
marg.hyper = t.marg(mod$marginals.hyperpar, function(x) (x), param.names = HP.names)
p.selection = c("Intercept", "dem", "Gamma precision", 
                "Time precision", "Rho for time", 
                "Range for spat.idx", "Stdev for spat.idx")

posts = marg.fixed %>% bind_rows(marg.hyper) %>% filter(Param %in% p.selection)
```

```{r posteriors, fig.width=16}
ggplot(posts, aes(x=x,y=y)) + geom_line() + 
  facet_wrap(vars(Param), scales = "free") + 
  theme(text = element_text(size = 20))  
```

### Components of prediction

```{r}
# Define points of interest to break out components of prediction
pred.trace = gdf_traces %>% mutate(color=as.factor(trace_ID))

# Get raw trend calculations and variable values at locations of interest
dat.pred = dat %>% filter(trace_ID %in% pred.trace$trace_ID) %>% mutate(color=as.factor(trace_ID))
Years = sort(unique(dat.train$Year))
loc.pred = st_coordinates(pred.trace) %x% rep(1, length(Years))
covar.pred = dat.pred %>% select(trace_ID, dem) %>%
  group_by(trace_ID) %>% filter(row_number()==1)
yr.mod = rep(Years - mean(Years), nrow(pred.trace))
st.pred = tibble(X=loc.pred[,1], Y=loc.pred[,2], Year = rep(Years, nrow(pred.trace)), 
                 dem=rep(covar.pred$dem, each=length(Years)))

# Project prediction locations onto mesh
proj.pred = inla.mesh.projector(mesh=mesh, loc = loc.pred)

# Extract spatial RE of trend and time RE at mesh nodes
spat.trend = inla.mesh.project(proj.pred, mod$summary.random$spat.idx$mean)
time.RE = rep(mod$summary.random$time$mean, nrow(pred.trace))

# Construct linear predictor for each spacetime point
lin.est = mod$summary.fixed$mean[1] + 
  mod$summary.fixed$mean[2]*st.pred$dem + spat.trend*yr.mod + time.RE

# Get model components for each st point
st.pred = st.pred %>% 
  mutate(fx.RE=mod$summary.fixed$mean[2]*st.pred$dem, spat.trend=spat.trend*yr.mod, 
         time.RE=time.RE, pred=exp(lin.est)) %>% 
  mutate(pred.sd=sqrt(pred^2/mod$summary.hyperpar$mean[1]), Source="model", 
         color=rep(pred.trace$color, each=length(Years)))

# Average data for each spatial location and add trend estimates
spat.pred = st.pred %>% select(-Year, -spat.trend, -Source, -time.RE) %>% 
  group_by(color) %>% summarize_all(mean, rm.na=TRUE)
spat.pred['trend'] = spat.trend[seq(1,length(spat.trend), 
                                    by=(length(spat.trend)/nrow(spat.pred)))]
```

```{r}
ggplot(spat.pred, aes(x=X,y=Y, color=pred)) + geom_point() + scale_color_viridis_c()
ggplot(spat.pred, aes(x=X,y=Y, color=fx.RE)) + geom_point() + scale_color_gradient2()
ggplot(spat.pred, aes(x=X,y=Y, color=trend)) + geom_point() + scale_color_gradient2()
```

Direct comparisons for individual trace time series between observed and predicted
```{r}
pred.comp = dat.pred %>% select(color, Year, accum, std, SMB) %>% 
  right_join(st.pred, by=c("color", "Year")) %>% 
  mutate(log.diff=log(pred/accum))

ggplot(pred.comp) + 
  geom_line(aes(x=Year, y=log.diff, group=color), size=0.25, alpha=0.1) + 
  theme(legend.position="none")
ggplot(pred.comp) + geom_abline(slope=1, color='black') + 
  geom_point(aes(x=accum, y=SMB), color='red', alpha=0.1) + 
  geom_point(aes(x=accum, y=pred), color='blue', alpha=0.1)
ggplot(pred.comp) + geom_density(aes(x=log(pred/accum)), fill='red', alpha=0.3)
```


```{r}
mod.time = tibble(Year=Years, mu=mod$summary.random$time$mean, 
                  sd=mod$summary.random$time$sd)
tmp = pred.comp %>% group_by(Year) %>% summarize(accum=mean(accum, na.rm=TRUE), pred=mean(pred))
mod.time = mod.time %>% mutate(obs.diff=log(tmp$accum/mean(tmp$accum)), 
                               pred.diff=log(tmp$pred/mean(tmp$pred)))
ggplot(mod.time) + geom_hline(yintercept = 0, color='black', linetype="dashed") + 
  geom_ribbon(aes(x=Year, ymin=mu-sd, ymax=mu+sd), fill='red', alpha=0.2) + 
  geom_line(aes(x=Year, y=mu), color='red') +
  geom_line(aes(x=Year, y=obs.diff), color='blue', alpha=0.5) + 
  geom_line(aes(x=Year, y=pred.diff), color='purple', alpha=0.5)
```

Calculate probability that trend is less than zero.
```{r}
P.neg = vector(mode = "numeric", length = length(mod$marginals.random$spat.idx))
for (i in 1:length(P.neg)) {
  marg.i = inla.tmarginal(function(x) x, 
                          mod$marginals.random$spat.idx[[i]], 
                          n=500)
  P.i = diff(marg.i[,1]) * marg.i[1:(nrow(marg.i)-1),2]
  P.neg[i] = sum(P.i[which(marg.i[1:(nrow(marg.i)-1),1]<0)])
}
tmp = tibble(Source='model', 
             East=mesh$loc[,1], North=mesh$loc[,2], 
             med=mod$summary.random$spat.idx$`0.5quant`, 
             P.neg=P.neg, 
             LB=mod$summary.random$spat.idx$`0.025quant`, 
             UB=mod$summary.random$spat.idx$`0.975quant`) %>% 
  st_as_sf(coords=c("East", "North"), crs=3031)
```

Map of estimated trends with probability of less than zero
```{r}
trends.comp = dat.mu %>% select(-trace_ID, -accum, -estimate)  %>% 
  rename(med=log.est) %>% 
  mutate(LB=NA, UB=NA, P.neg=NA, Source="data") %>% 
  st_as_sf(coords=c("East", "North"), crs=3031) %>%
  bind_rows(tmp)
```

```{r}
ggplot(trends.comp) + geom_density(aes(x=med, group=Source, color=Source)) + 
  xlim(c(-0.07,0.07))

plt_bnds = st_bbox(gdf_traces)
ggplot(trends.comp, aes(color=med)) + 
  geom_sf() + scale_color_gradient2() + 
  xlim(c(plt_bnds[1], plt_bnds[3])) + ylim(c(plt_bnds[2], plt_bnds[4])) + 
  facet_wrap(vars(Source))

ggplot(trends.comp %>% filter(Source!="data")) +
  geom_sf(aes(color=(UB-LB)/2)) + 
  xlim(c(plt_bnds[1], plt_bnds[3])) + ylim(c(plt_bnds[2], plt_bnds[4])) + 
  scale_color_viridis_c(option = "plasma", limits=c(0.003, 0.013))
  # scale_color_viridis_c(option = "plasma", trans="log")

ggplot(trends.comp %>% filter(Source!="data")) +
  geom_sf(aes(color=P.neg)) + 
  xlim(c(plt_bnds[1], plt_bnds[3])) + ylim(c(plt_bnds[2], plt_bnds[4])) + 
  scale_color_continuous_diverging(palette="Tropic", mid=0.50)

ggplot(trends.comp %>% filter(Source!="data")) + geom_density(aes(x=P.neg), fill='red', alpha=0.3)
```

```{r}
mod.trends = trends.comp %>% filter(Source=="model")
Neg.threshold = 0.95
frac.Neg = nrow(mod.trends %>% filter(P.neg>=Neg.threshold))/nrow(mod.trends)
print(paste(
  format(100*frac.Neg, digits=2), 
  "% of the region has at least ", 
  format(100*Neg.threshold, digits=2), 
  "% probability of a negative accumulation trend", 
  sep=''))

frac.Pos = nrow(mod.trends %>% filter(P.neg<=(1-Neg.threshold)))/nrow(mod.trends)
print(paste(
  format(100*frac.Pos, digits=2), 
  "% of the region has at least ", 
  format(100*Neg.threshold, digits=2), 
  "% probability of a positive accumulation trend", 
  sep=''))
```


